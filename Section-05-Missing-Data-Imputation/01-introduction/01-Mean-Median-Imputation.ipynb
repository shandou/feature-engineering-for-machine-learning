{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean / Median imputation\n",
    "\n",
    "[Feature Engineering for Machine Learning Course](https://www.trainindata.com/p/feature-engineering-for-machine-learning)\n",
    "\n",
    "Imputation is the act of replacing missing data with statistical estimates of the missing values. The goal of any imputation technique is to produce a **complete dataset** that can be used to train machine learning models.\n",
    "\n",
    "Mean/median imputation consists of replacing all occurrences of missing values (NA) within a variable by the mean (if the variable has a Gaussian distribution) or median (if the variable has a skewed distribution).\n",
    "\n",
    "**Note**:\n",
    "\n",
    "If a variable is normally distributed, the mean, median, and mode are approximately the same. Therefore, replacing missing values by the mean and the median are equivalent.\n",
    "\n",
    "If the variable is skewed, the mean is biased towards values at the end of the distribution. As a result, the median represents the majority of the values in the variable better.\n",
    "\n",
    "Replacing missing data by the mode is not common practice for numerical variables.\n",
    "\n",
    "### Which variables can I impute with mean or median imputation?\n",
    "\n",
    "The mean and median can only be calculated on numerical variables. Therefore, these methods are suitable for continuous and discrete numerical variables only.\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "- Data is missing completely at random.\n",
    "\n",
    "If data is missing at random, it's safe to assume that the missing values are extremely close to the mean or median of the distribution, which is the value of the majority of the observations.\n",
    "\n",
    "### Advantages\n",
    "\n",
    "- Fast way of obtaining complete datasets\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- Distortion of the original variable distribution (if the fraction of NA is large):\n",
    "\n",
    "- Distortion of the variance\n",
    "\n",
    "- Distortion of the covariance with other variables of the dataset\n",
    "\n",
    "If the proportion of NA is large, the variance of the variable will be distorted when it is replaced with the mean or median, resulting in an underestimating of the variance.\n",
    "\n",
    "Estimates of covariance and correlations with other variables may also be affected. Because the mean or median value that now substitutes the missing data does not always retain the relation with the remaining variables, mean or median imputation may change intrinsic correlations.\n",
    "\n",
    "Finally, \"concentrating\" missing values at the mean or median value may lead to observations that are common occurrences in the distribution being picked up as outliers.\n",
    "\n",
    "### When should mean or median imputation be used?\n",
    "\n",
    "- Data is missing completely at random\n",
    "\n",
    "- No more than 5% of the variable contains missing data\n",
    "\n",
    "Although in theory, the above conditions should be met to minimise the impact of this imputation technique, in practice, mean / median imputation is very commonly used, even when data is not MCAR and the fraction of missing values is large. The reason behind this, is the simplicity of the technique.\n",
    "\n",
    "### Final note\n",
    "\n",
    "Replacement of NA with mean or median is widely used in the data science community and in data science competitions. See, for example, the winning solution of the KDD 2009 cup: [\"Winning the KDD Cup Orange Challenge with Ensemble Selection\"](http://www.mtome.com/Publications/CiML/CiML-v3-book.pdf).\n",
    "\n",
    "Typically, mean and median imputation is done together with adding a \"missing indicator\" variable to mark observations where the data was missing (see lecture \"Missing Indicator\"), thus covering 2 angles: if the data was missing completely at random, this would be captured by the mean and median imputation, and if it wasn't, this would be captured by the additional \"missing indicator\" variable. Both methods are extremely straightforward to implement and therefore are a top choice for imputation.\n",
    "\n",
    "## Datasets:\n",
    "\n",
    "- Ames House Price\n",
    "- Titanic\n",
    "\n",
    "To download the datasets, please refer to the lecture **Datasets** in **Section 2** of this course.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# to split the datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titanic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>age</th>\n",
       "      <th>fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>211.3375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>151.5500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>151.5500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>151.5500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>25.0000</td>\n",
       "      <td>151.5500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived      age      fare\n",
       "0         1  29.0000  211.3375\n",
       "1         1   0.9167  151.5500\n",
       "2         0   2.0000  151.5500\n",
       "3         0  30.0000  151.5500\n",
       "4         0  25.0000  151.5500"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Titanic dataset with a few variables for demonstration.\n",
    "\n",
    "data = pd.read_csv(\"../../Datasets/titanic.csv\", usecols=[\"age\", \"fare\", \"survived\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "survived    0.000000\n",
       "age         0.200917\n",
       "fare        0.000764\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at the fraction of NA.\n",
    "\n",
    "data.isnull().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age and fare show missing data. If instead of `mean()`, we use `sum()`, we will see that fare only has 1 missing value.\n",
    "\n",
    "### Imputation important\n",
    "\n",
    "The imputation values (that is the mean/median) should be calculated using the training set, and the same value should be used to impute the test set. This is to avoid overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's separate into training and testing set.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data[[\"age\", \"fare\"]],  # predictors\n",
    "    data[\"survived\"],  # target\n",
    "    test_size=0.5,  # percentage of obs in test set\n",
    "    random_state=0,\n",
    ")  # seed to ensure reproducibility\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore the missing data in the train set.\n",
    "# The percentage of NA should be fairly similar to those\n",
    "# observed in the whole dataset.\n",
    "\n",
    "X_train.isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the median Age.\n",
    "\n",
    "median = X_train.age.median()\n",
    "median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean Age\n",
    "\n",
    "mean = X_train.age.mean()\n",
    "\n",
    "# The mean contains many decimals, so I round to 1\n",
    "# using the round function from numpy.\n",
    "mean = np.round(mean, 1)\n",
    "\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new variable with the missing values replaced\n",
    "# using the function we created above.\n",
    "\n",
    "# First, replace with the median.\n",
    "X_train[\"Age_median\"] = X_train[\"age\"].fillna(median)\n",
    "\n",
    "# Now replace with the mean.\n",
    "X_train[\"Age_mean\"] = X_train[\"age\"].fillna(mean)\n",
    "\n",
    "X_train.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the rows with missing data (NaN) in Age, and see how in the new variables those were replaced by either 28 (median) or 30 (mean).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see a change in the variance after mean / median imputation.\n",
    "# This is expected, because the percentage of missing data is quite\n",
    "# high in Age, ~20%.\n",
    "\n",
    "print(\"Original variable variance: \", X_train[\"age\"].var())\n",
    "print(\"Variance after median imputation: \", X_train[\"Age_median\"].var())\n",
    "print(\"Variance after mean imputation: \", X_train[\"Age_mean\"].var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the variance is underestimated because many values are now the same (i.e., the mean or the median value).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that the distribution has changed:\n",
    "# there are now more values towards the median\n",
    "# or median.\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# original variable distribution\n",
    "X_train[\"age\"].plot(kind=\"kde\", ax=ax)\n",
    "\n",
    "# variable imputed with the median\n",
    "X_train[\"Age_median\"].plot(kind=\"kde\", ax=ax, color=\"red\")\n",
    "\n",
    "# variable imputed with the mean\n",
    "X_train[\"Age_mean\"].plot(kind=\"kde\", ax=ax, color=\"green\")\n",
    "\n",
    "# add legends\n",
    "lines, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(lines, labels, loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned previously, the mean / median imputation distorted the original distribution of the variable Age. The transformed variable shows more values around the mean / median values.\n",
    "\n",
    "**Is this important?**\n",
    "\n",
    "It depends on the machine learning model you want to build. Distorting the variable distribution may affect its relationship with the target, which might be important for linear models. Therefore, the final imputed variable might negatively affect the linear model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also said that mean / median imputation may affect the relationship\n",
    "# with the other variables in the dataset.\n",
    "\n",
    "# Let's have a look.\n",
    "\n",
    "X_train[[\"fare\", \"age\", \"Age_median\", \"Age_mean\"]].cov()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariance between Age and Fare changed with the mean / median imputation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, I mentioned that mean / median imputation may lead\n",
    "# inliers to look like outliers.\n",
    "\n",
    "# In other words, mean / median imputation may lead to an increase\n",
    "# in the number of detected outliers.\n",
    "\n",
    "# Let's find out using a boxplot.\n",
    "X_train[[\"age\", \"Age_median\", \"Age_mean\"]].boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the boxplot, we can see that after the imputation, we not only have more outliers at the higher age values, but we now have outliers as well at the lower age values.\n",
    "\n",
    "**Is this important?**\n",
    "\n",
    "If we are after true outliers, we need to keep this behavior in mind to make sure that we are neither masking nor creating artificial outliers with our imputation technique. In practice, we normally don't check for this behaviour at all. But I think it is important to know that it is happening.\n",
    "\n",
    "## House Prices Dataset\n",
    "\n",
    "In this dataset, there are variables with a lot or with a few missing data points. Therefore, we can compare the effects of mean and median imputation in both cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the following variables,\n",
    "# 3 of which contain NA.\n",
    "\n",
    "cols_to_use = [\n",
    "    \"OverallQual\",\n",
    "    \"TotalBsmtSF\",\n",
    "    \"1stFlrSF\",\n",
    "    \"GrLivArea\",\n",
    "    \"WoodDeckSF\",\n",
    "    \"BsmtUnfSF\",\n",
    "    \"LotFrontage\",\n",
    "    \"MasVnrArea\",\n",
    "    \"GarageYrBlt\",\n",
    "    \"SalePrice\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the House Prices dataset.\n",
    "\n",
    "data = pd.read_csv(\"../../Datasets/houseprice.csv\", usecols=cols_to_use)\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the variables with missing data.\n",
    "\n",
    "# In the next list comprehension, I loop over all the columns\n",
    "# of the dataset and select those with missing data.\n",
    "\n",
    "[var for var in data.columns if data[var].isnull().sum() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find the percentage of missing data in those variables\n",
    "# (same code used in section 3).\n",
    "\n",
    "data[[\"LotFrontage\", \"MasVnrArea\", \"GarageYrBlt\"]].isnull().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember that the mean and the median that we will use to replace the NA are calculated using the train set.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's separate into training and testing sets.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(\"SalePrice\", axis=1),\n",
    "    data[\"SalePrice\"],\n",
    "    test_size=0.3,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the distributions of the variables.\n",
    "# We learned this code in section 3.\n",
    "\n",
    "X_train.hist(bins=50, figsize=(10, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new variable with missing values replaced\n",
    "# by the median using the function we created previously.\n",
    "\n",
    "# I repeat the following lines for each of the variables with NA\n",
    "# over the next cells.\n",
    "\n",
    "median = X_train[\"LotFrontage\"].median()\n",
    "\n",
    "X_train[\"LotFrontage_median\"] = X_train[\"LotFrontage\"].fillna(median)\n",
    "X_test[\"LotFrontage_median\"] = X_test[\"LotFrontage\"].fillna(median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median = X_train[\"MasVnrArea\"].median()\n",
    "\n",
    "X_train[\"MasVnrArea_median\"] = X_train[\"MasVnrArea\"].fillna(median)\n",
    "X_test[\"MasVnrArea_median\"] = X_test[\"MasVnrArea\"].fillna(median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median = X_train[\"GarageYrBlt\"].median()\n",
    "\n",
    "X_train[\"GarageYrBlt_median\"] = X_train[\"GarageYrBlt\"].fillna(median)\n",
    "X_test[\"GarageYrBlt_median\"] = X_test[\"GarageYrBlt\"].fillna(median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's evaluate the effect of median imputation on the distribution.\n",
    "\n",
    "# We can see that the distribution has changed for LotFrontAge,\n",
    "# there are more values towards the median.\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Original variable distribution.\n",
    "X_train[\"LotFrontage\"].plot(kind=\"kde\", ax=ax)\n",
    "\n",
    "# Variable imputed with the median.\n",
    "X_train[\"LotFrontage_median\"].plot(kind=\"kde\", ax=ax, color=\"red\")\n",
    "\n",
    "# Add legends.\n",
    "lines, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(lines, labels, loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For MasVnrArea, because the percentage of missing data\n",
    "# was low, we don't expect and won't see a dramatic change in the\n",
    "# distribution of the variable.\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# original variable distribution\n",
    "X_train[\"MasVnrArea\"].plot(kind=\"kde\", ax=ax)\n",
    "\n",
    "# variable imputed with the median\n",
    "X_train[\"MasVnrArea_median\"].plot(kind=\"kde\", ax=ax, color=\"red\")\n",
    "\n",
    "# add legends\n",
    "lines, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(lines, labels, loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Similarly, for GarageBlt, because the percentage of missing data\n",
    "# was low, we don't expect a big change in the\n",
    "# distribution. However, we see more values\n",
    "# around the median.\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# original variable distribution\n",
    "X_train[\"GarageYrBlt\"].plot(kind=\"kde\", ax=ax)\n",
    "\n",
    "# variable imputed with the median\n",
    "X_train[\"GarageYrBlt_median\"].plot(kind=\"kde\", ax=ax, color=\"red\")\n",
    "\n",
    "# add legends\n",
    "lines, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(lines, labels, loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also explore the effect of the imputation technique on\n",
    "# the variance.\n",
    "\n",
    "# We can see a change in the variance after imputation of LotFrontAge.\n",
    "# This is expected, because the percentage of missing data is quite\n",
    "# high ~20%.\n",
    "\n",
    "print(\"Original Variance: \", X_train[\"LotFrontage\"].var())\n",
    "print(\"Variance after median imputation: \", X_train[\"LotFrontage_median\"].var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is not the case for MasnVnrArea,\n",
    "# because the % of missing data was small.\n",
    "\n",
    "print(\"Original Variance: \", X_train[\"MasVnrArea\"].var())\n",
    "print(\"Variance after median imputation: \", X_train[\"MasVnrArea_median\"].var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For GarageBlt, the % of missing data was small, so the effect\n",
    "# on the variance will also be small.\n",
    "\n",
    "print(\"Original Variance: \", X_train[\"GarageYrBlt\"].var())\n",
    "print(\"Variance after median imputation: \", X_train[\"GarageYrBlt_median\"].var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, let's explore the covariance:\n",
    "# Take your time to compare the values in the table below.\n",
    "# See how the covariance is affected for LotFrontAge but not\n",
    "# so much for the other 2 variables.\n",
    "\n",
    "X_train.cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, let's explore the effect on outliers.\n",
    "\n",
    "# There is a decrease in the outlier boundaries for LotFrontage\n",
    "# (look at the height of the top whiskers),\n",
    "# but there is not a visible effect for the other 2 variables.\n",
    "\n",
    "X_train[[\"LotFrontage\", \"LotFrontage_median\"]].boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[[\"MasVnrArea\", \"MasVnrArea_median\"]].boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[[\"GarageYrBlt\", \"GarageYrBlt_median\"]].boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Later in this section, we will carry out mean / median imputation with open source Python packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feature_engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "556px",
    "left": "0px",
    "right": "1156.4px",
    "top": "107px",
    "width": "278px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
